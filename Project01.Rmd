---
title: "Group Project 1"
subtitle: "Biology 368/664 Bucknell University"
output: html_notebook
author: Derek Knight, Jessica Parrino, Margot Beck
date: 21 Feb 2022
---


## Target Audience

Bucknell 202 biology students interested in public health who are new to R and are looking to explore data analysis.  


## Grading

Each student will be expected to complete the following tasks to earn 85% of the points available for this assignment (21/25).

- Identify and obtain suitable dataset
- Use a Github repository and version control to collaborate on the project
- Spend 4-6 hours preparing, coding, and testing tutorial
  + Data exploration (Margot)
  + Data visualization (Derek)
  + Hypothesis testing
- Present tutorial in class
- Provide public archive suitable for sharing to students/faculty

Tutorials from previous classes can be viewed at our public github site: https://github.com/Bucknell-Biol364

Each group should use an *Acknowledgements* section to document the participation of each member and the collaboration within and between groups.

Additional credit will be awarded for providing assistance to other groups and for the development of a tutorial that goes beyond the minimal expectations listed above.

-----

#Coding in R for Bucknell BIOL 202 Students

## Introduction 
Welcome to coding in R, biology students! As scientists, you understand that data is everything when it comes to documenting the results of your biology labs, supporting or rejecting a hypothesis in an experiment, or understanding trends in public health (like perhaps transmission trends in a worldwide pandemic). Data doesn't speak for itself though, it requires a little bit of help on our part. What good does a spreadsheet of numbers, characters, or observations do on its own if you don't perform some analyses to make sense of it? That is where programs like R come in handy to scientists (or anyone working with datasets, for that matter) as important tools for data exploration, modeling, and analysis. By following along with this R tutorial, you will begin to see how useful coding can be when it comes to working with biological data.  

## About the Dataset
The data you will be using for this exercise is the NHANES 2009-2012 dataset. It is survey data from the US National Center for Health Statistics that interviews approximately 5,000 individuals. The interviewees also complete a health examination as part of the survey.  75 variables are included for the 2009-2010 and 2011-2012 sample years.  

## Download Packages and Obtain the Dataset 
First, you will need to download all required packages and download the NHANES dataset. Packages are essentially collections of functions and data sets. Run the first code chunk below by clicking the green arrow in the top right hand corner to install the packages. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require("NHANES")) install.packages("NHANES"); library(NHANES)
if (!require("UsingR")) install.packages("UsingR"); library(UsingR)
if (!require("cowplot")) install.packages("cowplot"); library(cowplot)
if (!require("datasauRus")) install.packages("datasauRus"); library(datasauRus)
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)
```

Next, run the below chunk of code line by line.  You can do this by hitting "Ctrl+Enter" on each line. You can also click the green arrow in the upper right hand corner of grey box. The first line will load the dataset "NHANES" into your Environment tab (upper right hand window of R-studio). The second line will open up an info page with more information about the NHANES dataset in the Help tab of the lower right hand window of R-studio.

```{r NHANES}
data("NHANES")
help(NHANES)
```

# Data Exploration
Now that you have the relevant packages installed and the dataset downloaded, it's time to get an idea of what the data are made up of. Run the below code chunk line by line to explore the data. The first function, "summary", will provide you with a summary of the dataset which includes the minimum, median, mean, maximum, 1st quartile, 3rd quartile, and NA's for each variable. In the next two lines, the function "head" will show you the first 6 rows of each variable while "tail" will show you the last 6. In the fourth line of code, the function "dim" will display the dimensions of the data table in the form of 'row, column'. This code chunk should help to give you an idea of what variables make up the data, the size of the dataset, and what the range of observations are for each variable.  

```{r Data Exploration}
summary(NHANES)
head(NHANES)
tail(NHANES)
dim(NHANES)
```

You can also look at the summary of just a single column. You can type the dataset (in this case NHANES) followed by a dollar sign with the column you are interested (in this case MaritalStatus.
```{r}
summary(NHANES$MaritalStatus)
head(NHANES$MaritalStatus)
tail(NHANES$MaritalStatus)
```

Let's try to write code youself! Look for teh summary of just the column Race1:
```{r}

```


# Data Visualization

After exploring the dataset, it is essential to visualize it. Here is a silly example that Professor Field showed us in our Data Analysis course. If you look at the mean of x, mean of y, standard deviation of x, and standard deviation of y, and correlation between x and y, all of the data looks similar. 

```{r}
datasaurus_dozen %>% 
  group_by(dataset) %>% 
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
      )
```

However, upon visualizing the data, we can see there are distinct patterns for each.

```{r fig.height=12, fig.width=9}
  ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 3)
```

To visualize the NHANES dataset, we can look at the spread of a numeric variable using a jitterplot and a boxplot. The boxplot provides information on the sumarry statistics, and the jitterplot shows where each individual data point lies. We can use boxplots and jitterplots to visualize the differences in the numeric variable for different categorical variables. For this example, we are looking at height for the various races. Now look at the code. you can see we are using a command from the ggplot2 package. We are making Race the x-axis and Height the y-axis. We have a boxplot and a jitterplot on the graph (the size command changes the size of each point and alpha changes how dark each point is). We are suing the cowplot package theme, and we titled the x and y axis. 
```{r}
ggplot(NHANES) +
  aes(x = Race1, y = Height) +
  geom_boxplot() +
  geom_jitter(color="black", size=0.1, alpha=0.2) +
  theme_cowplot() +
  xlab("Race") + 
  ylab("Height (cm)")
```

Now, let's try making a boxplot for yourself. Try to make boxplots of BMI for the two different genders below.
```{r}

```


We can also visualize the difference between two numeric variables. For this, we can make a scatterplot. Looking at the code, we put Height on the x-axis and Pulse on the y-axis. We can change the points size and transparency (alpha), and label different axis as well. 
```{r}
ggplot(NHANES, aes(x = Height, y = Pulse)) +
  geom_point(aes(colour = "black"),
             position = "jitter", 
             alpha = 0.8,
             size=0.05) +
  xlab("Pulse") + 
  ylab("Height")
  theme_cowplot()
```

You can also color code the data points to change by gender. You can find the code for that below. Here, we told it to change the color of the datapoints based on Gender.
```{r}
ggplot(NHANES, aes(x = Height, y = Pulse)) +
  geom_point(aes(colour = factor(Gender)), 
             position = "jitter", 
             alpha = 0.8,
             size=0.05) +
  xlab("Pulse") + 
  ylab("Height")
  theme_cowplot()
```

You can also add a linear regression trendline to the graph. You can see the code for that below. We added a geom_smooth command with the method "lm", meaning linear model. We had the colors of the linear model lines match the gender.
```{r}
ggplot(NHANES, aes(x = Height, y = Pulse)) +
  geom_point(aes(colour = factor(Gender)), 
             position = "jitter", 
             alpha = 0.8,
             size=0.05) +
  geom_smooth(method='lm',aes(group = factor(Gender), 
                              colour = factor(Gender))) + 
  xlab("Pulse") + 
  ylab("Height")
  theme_cowplot()
```

Let's have you try a graph! Make a scatterplot with BMI and pulse below:
```{r}

```

Now, use that code to separate the scatterplot with BMI and pulse based on gender:
```{r}

```

Now, take that line of code for the scatterplot with BMI and pulse and add linear model lines for the two genders:
```{r}

```


# Hypothesis Testing
Now that we have explored and visualized the data, we can form hypothesis and test them in order to see how (if at all) our data is correlated. For example, lets look at two quantitative variables SleepHrsNight (self-reported number of hours study participant usually gets at night on weekdays or workdays, reported for participants aged 16 years and older) and PhysActiveDays (Number of days in a typical week that participant does moderate or vigorous-intensity activity, reported for participants 12 years or older). 

For this example, the null hypothesis is there is no correlation between number of hours slept per night and exercise. The alternative hypothesis would be number of hours slept per night is correlated to exercise. 

## Shapiro Testing
In order to run this hypothesis test, we want to make sure our data is normally distributed. If the value of p is equal to or less than 0.05, then the hypothesis of normality will be rejected by the Shapiro test. Run the code chunk that tests the normality of each variable in our hypothesis. 
```{r}
shapiro.test(NHANES$PhysActiveDays)

```

## Transforming the data
We can see that our p-value is very small, so normality is rejected. In order to normalize the data, we can take the log transformation. The histogram function allows you to visualize the log transformed data a see that it is now normally distributed. 

```{r}
log_PhysActiveDays <- log(NHANES$PhysActiveDays)
hist(log(NHANES$PhysActiveDays))

```
## Linear Model
Now that the data is normally distributed, we can run what is called a linear model in order to reject or accept our hypothesis. Linear models describe a continuous response variable as a function of one or more predictor variables. It is a regression model that uses a straight line to describe the relationship between variables (in this case the relationship between hours slept and exercise). It finds the line of best fit through your data by searching for the value of the regression coefficient(s) that minimizes the total error of the model. The next code chunk is telling R to analyze if hours spent exercising effects hours slept. Run the next chunk and pay close attention to it's p-value. 
```{r}
lmPhysActiveDays <- lm(SleepHrsNight~log_PhysActiveDays, data=NHANES)
summary(lmPhysActiveDays)

```

As we can see our P-value is 0.212, therefore we cannot say that the number of days spent exercising is correlated to the number of hours slept. This is okay because now we gain more knowledge about our data and can begin to form and test new hypotheses!

# Conclusion
This tutorial was created in order to begin to get you to think about and familiarize yourself with R and data exploration. We took quite a large data set, explored and visualized it, formulated a hypothesis and assigned meaning to a large quantity of data. It is okay if your hypotheses are incorrect, or the outcome is not what you predicted. This outcome is still valuable because you gain more information about your dataset, and can formulate new questions and hypotheses to explore and verify.

# Acknowledgements
The information about the NHANES dataset was found on the 'help' page for NHANES in R.

Margot Beck wrote the introduction and data exploration sections.  

Code used for exploring the data was inspired by a blog post on R-bloggers (https://www.r-bloggers.com/2018/11/explore-your-dataset-in-r/).

Code for hypothesis testing was inspired by Homeworks 1 and 2. 

Jessica Parrino wrote the hypothesis testing and conclusion sections. 

Derek Knight wrote the Data Visualization section and the inserts of open code for the student to try.

DatasauRus package and description below: Stephanie Locke https://github.com/jumpingrivers/datasauRus

The datasauRus package wraps the awesome Datasaurus Dozen dataset, which contains 13 sets of x-y data. Each sub-dataset has five statistics that are (almost) the same in each case. (These are the mean of x, mean of y, standard deviation of x, standard deviation of y, and Pearson correlation between x and y). However, scatter plots reveal that each sub-dataset looks very different. The dataset is intended to be used to teach students that it is important to plot their own datasets, rather than relying only on statistics.

The Datasaurus was created by Alberto Cairo in this great [blog post](http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html).

He's been subsequently made even more famous in the paper [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats) by Justin Matejka and George Fitzmaurice.

In the paper, Justin and George simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions. 

This package also makes these datasets available for use as an advanced [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet), available in R as `anscombe`.
